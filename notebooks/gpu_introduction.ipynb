{"cells":[{"cell_type":"markdown","metadata":{"id":"6A-Cdf97yxbM"},"source":["![img](https://github.com/JuliaLang/julia/raw/master/doc/src/assets/logo.svg)![img](https://avatars.githubusercontent.com/u/7346142?s=200&v=4)"]},{"cell_type":"markdown","metadata":{"id":"a3RZC3JpyxbN"},"source":["# Installation and Setup"]},{"cell_type":"markdown","metadata":{"id":"pAh2gPHbyxbN"},"source":["JuliaGPU packages are easy to install: Just do `Pkg.add(\"CUDA\")` to install the CUDA.jl package, which provides bindings to NVIDIA's CUDA. CUDA.jl provides all of the compiler and runtime logic needed to program NVIDIA GPUs; the only thing you need to provide is a functional NVIDIA driver (which most HPC systems already have installed and configured), but you don't need to install the CUDA toolkit! CUDA.jl downloads one if it's not already available on your system (again, HPC systems usually have this provided for you):"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"c_2ALEg1yxbO","executionInfo":{"status":"ok","timestamp":1744810406105,"user_tz":240,"elapsed":157365,"user":{"displayName":"Rabab Alomairy","userId":"17426069845971301739"}},"outputId":"23460b78-b172-461a-bd3b-7ff76874c8fc"},"outputs":[{"output_type":"stream","name":"stderr","text":["\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m registry at `~/.julia/registries/General.toml`\n","\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n","\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.10/Project.toml`\n","\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.10/Manifest.toml`\n"]}],"source":["# This can take a little while to download and compile, so just be patient\n","import Pkg\n","Pkg.add(\"CUDA\")"]},{"cell_type":"markdown","metadata":{"id":"qbHdpW4pyxbQ"},"source":["We're going to be running some small benchmarks in this notebook, so let's also grab Julia's BenchmarkTools.jl while we're at it:"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"9Ak7on5OyxbQ","executionInfo":{"status":"ok","timestamp":1744811281278,"user_tz":240,"elapsed":13593,"user":{"displayName":"Rabab Alomairy","userId":"17426069845971301739"}},"outputId":"8f7065fe-db6c-4e55-c5ec-0e19e5ae2ca1"},"outputs":[{"output_type":"stream","name":"stderr","text":["\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n","\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m BenchmarkTools ─ v1.6.0\n","\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.10/Project.toml`\n","  \u001b[90m[6e4b80f9] \u001b[39m\u001b[92m+ BenchmarkTools v1.6.0\u001b[39m\n","\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.10/Manifest.toml`\n","  \u001b[90m[6e4b80f9] \u001b[39m\u001b[92m+ BenchmarkTools v1.6.0\u001b[39m\n","  \u001b[90m[9abbd945] \u001b[39m\u001b[92m+ Profile\u001b[39m\n","\u001b[32m\u001b[1mPrecompiling\u001b[22m\u001b[39m packages...\n","   1930.1 ms\u001b[32m  ✓ \u001b[39mBenchmarkTools\n","  1 dependency successfully precompiled in 3 seconds. 460 already precompiled.\n"]}],"source":["import Pkg\n","Pkg.add(\"BenchmarkTools\")"]},{"cell_type":"markdown","metadata":{"id":"gzRc1RlryxbR"},"source":["And now we import these packages, along with the built-in LinearAlgebra standard library:"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"xCsDNoCGyxbR"},"outputs":[],"source":["using CUDA\n","using BenchmarkTools\n","\n","using LinearAlgebra"]},{"cell_type":"markdown","metadata":{"id":"eldik1_wyxbR"},"source":["Now, GPU vendor libraries can be difficult, so CUDA.jl provides a convenient way to check if everything is setup correctly, the `CUDA.versioninfo()` function. Like Julia's `Base.versioninfo()`, this will print some information on the available hardware and loaded libraries:"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"fsI4_5ClyxbS","executionInfo":{"status":"ok","timestamp":1744811292268,"user_tz":240,"elapsed":1482,"user":{"displayName":"Rabab Alomairy","userId":"17426069845971301739"}},"outputId":"8250e5b0-2c69-4e9c-a6ff-eb6f7f06e562"},"outputs":[{"output_type":"stream","name":"stdout","text":["CUDA runtime 12.5, local installation\n","CUDA driver 12.8\n","NVIDIA driver 550.54.15\n","\n","CUDA libraries: \n","- CUBLAS: 12.5.3\n","- CURAND: 10.3.6\n","- CUFFT: 11.2.3\n","- CUSOLVER: 11.6.3\n","- CUSPARSE: 12.5.1\n","- CUPTI: 2024.2.1 (API 23.0.0)\n","- NVML: 12.0.0+550.54.15\n","\n","Julia packages: \n","- CUDA: 5.7.2\n","- CUDA_Driver_jll: 0.12.1+1\n","- CUDA_Runtime_jll: 0.16.1+0\n","- CUDA_Runtime_Discovery: 0.3.5\n","\n","Toolchain:\n","- Julia: 1.10.9\n","- LLVM: 15.0.7\n","\n","Preferences:\n","- CUDA_Runtime_jll.version: 12.5.1\n","- CUDA_Runtime_jll.local: true\n","\n","1 device:\n","  0: NVIDIA A100-SXM4-40GB (sm_80, 39.552 GiB / 40.000 GiB available)\n"]}],"source":["CUDA.versioninfo()"]},{"cell_type":"markdown","metadata":{"id":"9tQ4LfITyxbS"},"source":["It's always good practice to check this at least once on a new system or when you mess with `module`s loaded, just to ensure that everything is connected and happy!"]},{"cell_type":"markdown","metadata":{"id":"aAc0-csDyxbT"},"source":["# Example: Vector Addition"]},{"cell_type":"markdown","metadata":{"id":"izjk19DKyxbT"},"source":["As a simple example, let's take a look at vector addition. Let's assume you have two vectors $\\vec{a}$ and $\\vec{b}$ and you want to add them elementwise. You can do this in many ways in Julia:\n","1. simple for loop on a CPU\n","2. julia array add (+) on a CPU or GPU\n","3. GPU kernel programming in CUDA (or KernelAbstractions using CUDA as backend - we'll see this soon!)"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"sizyI0plyxbT","executionInfo":{"status":"ok","timestamp":1744811297213,"user_tz":240,"elapsed":1819,"user":{"displayName":"Rabab Alomairy","userId":"17426069845971301739"}},"outputId":"9b531409-72cf-4718-c6ef-54e4582c4383"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["1024-element Vector{Int64}:\n"," 3\n"," 4\n"," 4\n"," 3\n"," 3\n"," 1\n"," 4\n"," 2\n"," 1\n"," 3\n"," 3\n"," 1\n"," 3\n"," ⋮\n"," 4\n"," 4\n"," 1\n"," 3\n"," 1\n"," 2\n"," 2\n"," 3\n"," 2\n"," 4\n"," 1\n"," 2"]},"metadata":{},"execution_count":5}],"source":["# define our input a, b vectors, and output c vector in CPU RAM\n","vector_size = 1024\n","a = rand(1:4, vector_size)\n","b = rand(1:4, vector_size)\n","c = zeros(Int, vector_size)\n","\n","# what's in a?\n","a"]},{"cell_type":"markdown","metadata":{"id":"CckwUEVjyxbU"},"source":["Let's write a simple CPU loop to add two vectors in serial:"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"CfmmorZByxbU","executionInfo":{"status":"ok","timestamp":1744811301619,"user_tz":240,"elapsed":4,"user":{"displayName":"Rabab Alomairy","userId":"17426069845971301739"}},"outputId":"12564e91-5671-4b96-b6c2-450d19d91c82"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["1024-element Vector{Int64}:\n"," 7\n"," 8\n"," 8\n"," 7\n"," 4\n"," 5\n"," 6\n"," 4\n"," 3\n"," 6\n"," 5\n"," 2\n"," 7\n"," ⋮\n"," 6\n"," 7\n"," 4\n"," 6\n"," 4\n"," 3\n"," 4\n"," 5\n"," 5\n"," 8\n"," 4\n"," 6"]},"metadata":{},"execution_count":6}],"source":["# Note: the exclamation mark (!) doesn't do anything special\n","# It's just used to indicate that a function mutates its arguments\n","function vadd!(a, b, c)\n","    for i in 1:length(c)\n","        c[i] = a[i] + b[i]\n","    end\n","end\n","vadd!(a, b, c)\n","c"]},{"cell_type":"markdown","metadata":{"id":"iWb2vLEqyxbU"},"source":["Thankfully, Julia has a ton of built-in array operations, so we don't actually need to implement this ourselves. Julia's vector add (+) operation works exactly as you'd expect:"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"4CSB9D14yxbV","executionInfo":{"status":"ok","timestamp":1744811305147,"user_tz":240,"elapsed":51,"user":{"displayName":"Rabab Alomairy","userId":"17426069845971301739"}},"outputId":"03c783e7-4449-4532-a9d5-9de4c2a1e245"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["1024-element Vector{Int64}:\n"," 7\n"," 8\n"," 8\n"," 7\n"," 4\n"," 5\n"," 6\n"," 4\n"," 3\n"," 6\n"," 5\n"," 2\n"," 7\n"," ⋮\n"," 6\n"," 7\n"," 4\n"," 6\n"," 4\n"," 3\n"," 4\n"," 5\n"," 5\n"," 8\n"," 4\n"," 6"]},"metadata":{},"execution_count":7}],"source":["c = a + b\n","\n","# Note that, unlike `vadd!`, the above operation allocates a new `c` as the output vector - this is important to remember."]},{"cell_type":"markdown","metadata":{"id":"jj0IdfmNyxbV"},"source":["Great! But isn't this a GPU tutorial? Let's get to it!"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"-MGeIqGDyxbV","executionInfo":{"status":"ok","timestamp":1744811315669,"user_tz":240,"elapsed":8169,"user":{"displayName":"Rabab Alomairy","userId":"17426069845971301739"}},"outputId":"812d846c-68e7-4c87-cb7e-3c67bead99c9"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["1024-element CuArray{Int64, 1, CUDA.DeviceMemory}:\n"," 3\n"," 4\n"," 4\n"," 3\n"," 3\n"," 1\n"," 4\n"," 2\n"," 1\n"," 3\n"," 3\n"," 1\n"," 3\n"," ⋮\n"," 4\n"," 4\n"," 1\n"," 3\n"," 1\n"," 2\n"," 2\n"," 3\n"," 2\n"," 4\n"," 1\n"," 2"]},"metadata":{},"execution_count":8}],"source":["# We need first to make copies of the a and b vectors on the GPU, and define a new dc empty GPU vector\n","\n","# The CuArray() function automatically allocates a new GPU array of the same size and shape as the input,\n","# and copies from the input CPU array to the newly-allocated GPU array\n","da = CuArray(a)\n","db = CuArray(b)\n","\n","# CUDA.zeros takes the desired element type and array size, and automatically allocates and initializes\n","# a new GPU array with int64 zeros\n","dc = CUDA.zeros(Int, size(a))\n","\n","# We can also safely take a look at what's in da, even though it's on the GPU!\n","# It's a different array type, and that fact is made clear to us:\n","da"]},{"cell_type":"markdown","metadata":{"id":"yTKq7A9FyxbW"},"source":["Now that we know how to allocate on the GPU, let's see how to use this same add (+) operation on the GPU to add two of those vectors using CUDA:"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"agHQghbGyxbW","executionInfo":{"status":"ok","timestamp":1744811316630,"user_tz":240,"elapsed":960,"user":{"displayName":"Rabab Alomairy","userId":"17426069845971301739"}},"outputId":"66763f6d-9c2b-4454-be56-af91544b1b7f"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["1024-element CuArray{Int64, 1, CUDA.DeviceMemory}:\n"," 7\n"," 8\n"," 8\n"," 7\n"," 4\n"," 5\n"," 6\n"," 4\n"," 3\n"," 6\n"," 5\n"," 2\n"," 7\n"," ⋮\n"," 6\n"," 7\n"," 4\n"," 6\n"," 4\n"," 3\n"," 4\n"," 5\n"," 5\n"," 8\n"," 4\n"," 6"]},"metadata":{},"execution_count":9}],"source":["dc = da + db\n","\n","# We can add GPU vectors using the same `+` operator, thanks to Julia's multiple dispatch!\n","# Also, like for the CPU add operation, this one also allocates a new GPU array for output `dc`"]},{"cell_type":"markdown","metadata":{"id":"A1UdjEmWyxbW"},"source":["Cool, but vector addition is pretty... simple? Let us learn how to write our own GPU kernels with CUDA.jl in pure Julia.\n","\n","In array operations, CUDA.jl can leverage implicit parallelism (expressed over the array's elements) to automatically execute these operations in parallel on a GPU. When using hand-rolled kernels, it is instead the programmer's responsibility to decide how to effectively assign the available parallel execution resources for the specific operation. Let's see how this is done for vector addition, before moving on to more interesting examples:"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"ckAM9n0KyxbX","executionInfo":{"status":"ok","timestamp":1744811320492,"user_tz":240,"elapsed":191,"user":{"displayName":"Rabab Alomairy","userId":"17426069845971301739"}},"outputId":"832abc28-e7ab-4903-9e8d-e37ab8ae11e3"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["vadd_kernel! (generic function with 1 method)"]},"metadata":{},"execution_count":10}],"source":["function vadd_kernel!(c, a, b)\n","    # Obtain GPU thread index, which should be mapped to the valid indices of a and b\n","    i = threadIdx().x\n","    # Each thread will add its own element to c\n","    c[i] = a[i] + b[i]\n","\n","    # GPU kernels don't return anything\n","    return\n","end"]},{"cell_type":"markdown","metadata":{"id":"8xyqMdBbyxbX"},"source":["At a high level, that's pretty easy, you just need to write a scalar function, just like you'd do if you were writing CUDA C++. Now we just need to launch that function in parallel using the `@cuda` macro, and specify the number of GPU threads with the `threads` keyword argument:"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"hwTmE9EryxbX","executionInfo":{"status":"ok","timestamp":1744811323724,"user_tz":240,"elapsed":1077,"user":{"displayName":"Rabab Alomairy","userId":"17426069845971301739"}},"outputId":"28a60843-13a5-411d-9633-5925cae8a19b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["1024-element CuArray{Int64, 1, CUDA.DeviceMemory}:\n"," 7\n"," 8\n"," 8\n"," 7\n"," 4\n"," 5\n"," 6\n"," 4\n"," 3\n"," 6\n"," 5\n"," 2\n"," 7\n"," ⋮\n"," 6\n"," 7\n"," 4\n"," 6\n"," 4\n"," 3\n"," 4\n"," 5\n"," 5\n"," 8\n"," 4\n"," 6"]},"metadata":{},"execution_count":11}],"source":["# Launch our `vadd_kernel!` GPU kernel, with our GPU arrays as inputs\n","@cuda threads=length(da) vadd_kernel!(dc, da, db)\n","\n","dc"]},{"cell_type":"markdown","metadata":{"id":"6KDF0RyeyxbY"},"source":["OK, this is great, and was not a lot of work for us! But to see a downside of this simple approach, let's try to work with bigger vectors, by setting `vector_size` to 10240:"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"colab":{"base_uri":"https://localhost:8080/","height":619},"id":"ScjyEbJ6yxbY","executionInfo":{"status":"error","timestamp":1744811325735,"user_tz":240,"elapsed":313,"user":{"displayName":"Rabab Alomairy","userId":"17426069845971301739"}},"outputId":"ec36e44e-df38-409d-f3d7-d7f2b7af8495"},"outputs":[{"output_type":"error","ename":"LoadError","evalue":"Number of threads in x-dimension exceeds device limit (10240 > 1024).","traceback":["Number of threads in x-dimension exceeds device limit (10240 > 1024).","","Stacktrace:","  [1] error(s::String)","    @ Base ./error.jl:35","  [2] diagnose_launch_failure(f::CuFunction, err::CuError; blockdim::CuDim3, threaddim::CuDim3, shmem::Int64)","    @ CUDA ~/.julia/packages/CUDA/TW8fL/lib/cudadrv/execution.jl:103","  [3] launch(::CuFunction, ::CUDA.KernelState, ::CuDeviceVector{Int64, 1}, ::CuDeviceVector{Int64, 1}, ::CuDeviceVector{Int64, 1}; blocks::Int64, threads::Int64, cooperative::Bool, shmem::Int64, stream::CuStream)","    @ CUDA ~/.julia/packages/CUDA/TW8fL/lib/cudadrv/execution.jl:79","  [4] launch","    @ ~/.julia/packages/CUDA/TW8fL/lib/cudadrv/execution.jl:58 [inlined]","  [5] #996","    @ ~/.julia/packages/CUDA/TW8fL/lib/cudadrv/execution.jl:195 [inlined]","  [6] macro expansion","    @ ~/.julia/packages/CUDA/TW8fL/lib/cudadrv/execution.jl:155 [inlined]","  [7] macro expansion","    @ ./none:0 [inlined]","  [8] convert_arguments","    @ ./none:0 [inlined]","  [9] #cudacall#995","    @ ~/.julia/packages/CUDA/TW8fL/lib/cudadrv/execution.jl:197 [inlined]"," [10] cudacall","    @ ~/.julia/packages/CUDA/TW8fL/lib/cudadrv/execution.jl:193 [inlined]"," [11] macro expansion","    @ ~/.julia/packages/CUDA/TW8fL/src/compiler/execution.jl:272 [inlined]"," [12] macro expansion","    @ ./none:0 [inlined]"," [13] #_#1183","    @ ./none:0 [inlined]"," [14] top-level scope","    @ ~/.julia/packages/CUDA/TW8fL/src/compiler/execution.jl:114"]}],"source":["vector_size = 10240\n","da = CuArray(rand(1:4, vector_size))\n","db = CuArray(rand(1:4, vector_size))\n","dc = CUDA.zeros(Int, vector_size)\n","\n","@cuda threads=length(da) vadd_kernel!(dc, da, db)"]},{"cell_type":"markdown","metadata":{"id":"z_8fL3BJyxbY"},"source":["Oh no! What is going on here?"]},{"cell_type":"markdown","metadata":{"id":"rSdKotAjyxbY"},"source":["GPUs have a limited number of threads they can run on a single streaming multiprocessor (SM) at once, and we just tried to assign too many threads to one SM, which isn't possible:"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"9CrM3u7ryxbY","executionInfo":{"status":"ok","timestamp":1744811331836,"user_tz":240,"elapsed":239,"user":{"displayName":"Rabab Alomairy","userId":"17426069845971301739"}},"outputId":"1b3d60ed-ee2a-4089-a31f-3ee061fd52aa"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["1024"]},"metadata":{},"execution_count":13}],"source":["# To query the number of threads per block, we can inspect CUDA attributes:\n","CUDA.attribute(device(), CUDA.DEVICE_ATTRIBUTE_MAX_THREADS_PER_BLOCK)"]},{"cell_type":"markdown","metadata":{"id":"hyBZvG8fyxbY"},"source":["Since 10240 > 1024, the SM wouldn't have had enough resources to satisfy our request, at least not with the default of a single block per kernel.\n","\n","Thankfully, GPUs also have multiple SMs, so in theory this should be solvable. To take advantage of more than one SM, we need to run a kernel with multiple blocks, as a single block can only execute on one SM (which has limited resources available, as we saw in the query above). In order to exploit multiple blocks, though, we need to understand how to index into our arrays when our index depends not just on our thread index, but also on our block index and block sizes."]},{"cell_type":"markdown","metadata":{"id":"f3yXNExsyxbZ"},"source":["In CUDA.jl, the expression `i = threadIdx().x + (blockIdx().x - 1) * blockDim().x` calculates a unique index for each thread across multiple blocks in a CUDA kernel execution. Here's a breakdown of each component and how they contribute to computing this index:\n","\n","- `threadIdx().x`: This returns the x-coordinate of the thread within its block. It's the thread's index within the block, starting from 1 (unlike C/C++ CUDA where it starts from 0).\n","\n","- `blockIdx().x`: This gives the x-coordinate of the block within the grid. It represents the block's index in the grid, also starting from 1.\n","\n","- `blockDim().x`: This represents the number of threads per block along the x-axis.\n","\n","In essence, the formula `i = threadIdx().x + (blockIdx().x - 1) * blockDim().x` is used to compute a global index for each thread, regardless of how blocks are sized. It positions the threads linearly across all blocks. Here's what each part does:\n","\n","- `(blockIdx().x - 1) * blockDim().x`: This part calculates the offset to the start of the current block. Subtracting 1 from `blockIdx().x` makes it zero-based, and then it is multiplied by the number of threads in each block `(blockDim().x)`. This gives the index of the first thread in the current block relative to the entire grid.\n","\n","- `threadIdx().x`: Adding this to the block offset gives the specific thread's index within the whole grid."]},{"cell_type":"markdown","metadata":{"id":"-4O79h1qyxbZ"},"source":["Knowing this, let's now rewrite our kernel to properly handle multiple blocks, using the above global indexing formula:"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"YhVVazfEyxbZ","executionInfo":{"status":"ok","timestamp":1744811336624,"user_tz":240,"elapsed":15,"user":{"displayName":"Rabab Alomairy","userId":"17426069845971301739"}},"outputId":"fb1dd925-7bd5-4d2b-824b-2e309cdd96ac"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["vadd_cuda! (generic function with 1 method)"]},"metadata":{},"execution_count":14}],"source":["function vadd_cuda!(c, a, b)\n","    # Calculate a unique index for each thread across multiple blocks\n","    i = threadIdx().x + (blockIdx().x - 1) * blockDim().x\n","\n","    # Ensure that we skip invalid indices, if we over-allocated a few threads\n","    if i <= length(a)\n","        c[i] = a[i] + b[i]\n","    end\n","\n","    return\n","end"]},{"cell_type":"markdown","metadata":{"id":"0awqU0KsyxbZ"},"source":["Now we can launch our kernel with the maximum number of threads per block (1024), and then divide up our computation across multiple 1024-wide blocks:"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"YCit5ESRyxba","executionInfo":{"status":"ok","timestamp":1744811339036,"user_tz":240,"elapsed":331,"user":{"displayName":"Rabab Alomairy","userId":"17426069845971301739"}},"outputId":"65ee784a-0e82-45d9-b0c1-c71eaf5c2bd0"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["10240-element CuArray{Int64, 1, CUDA.DeviceMemory}:\n"," 4\n"," 4\n"," 3\n"," 5\n"," 7\n"," 5\n"," 4\n"," 7\n"," 4\n"," 2\n"," 8\n"," 5\n"," 5\n"," ⋮\n"," 5\n"," 8\n"," 4\n"," 5\n"," 5\n"," 3\n"," 8\n"," 3\n"," 4\n"," 6\n"," 3\n"," 6"]},"metadata":{},"execution_count":15}],"source":["@cuda threads=1024 blocks=cld(length(da),1024) vadd_cuda!(dc, da, db) # cld(x, y) is (x / y) with round-up behavior\n","dc"]},{"cell_type":"markdown","metadata":{"id":"iOk9Q56Byxba"},"source":["Yay! Now that we're thoroughly done with vector addition, let's move on to something a bit heavier:"]},{"cell_type":"markdown","metadata":{"id":"mg4-GXKMyxba"},"source":["## Example: Matrix-Matrix Multiplication"]},{"cell_type":"markdown","metadata":{"id":"6vpc_z4ayxba"},"source":["Matrix multiplication is a mainstay of all kinds of applications, so we should be able to implement this in Julia with ease. Let's first take a look at doing this on the CPU:"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"N7uqP3Xwyxba","executionInfo":{"status":"ok","timestamp":1744811342137,"user_tz":240,"elapsed":583,"user":{"displayName":"Rabab Alomairy","userId":"17426069845971301739"}},"outputId":"b8b3ecab-5755-4eda-a658-55fd26e1da92"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["1024×1024 Matrix{Float64}:\n"," 0.111411  0.0846486  0.0497382  …  0.613677     0.377845    0.0341445\n"," 0.174748  0.350421   0.227078      0.671424     0.141305    0.147167\n"," 0.731431  0.185005   0.503551      0.148479     0.880455    0.0922076\n"," 0.690734  0.108755   0.705794      0.825014     0.0326488   0.471357\n"," 0.84956   0.149493   0.163321      0.71118      0.736062    0.747991\n"," 0.785406  0.270562   0.0286078  …  0.903958     0.225837    0.322484\n"," 0.33145   0.667579   0.498192      0.442454     0.24898     0.910926\n"," 0.763405  0.352141   0.958001      0.000287206  0.353318    0.513929\n"," 0.684119  0.0739164  0.41486       0.47827      0.875003    0.61574\n"," 0.529939  0.792093   0.336086      0.595774     0.529696    0.320064\n"," 0.99392   0.917188   0.309304   …  0.230307     0.189118    0.443715\n"," 0.250578  0.451906   0.962513      0.174126     0.885104    0.569134\n"," 0.946768  0.228864   0.445785      0.279041     0.00986295  0.569224\n"," ⋮                               ⋱                           \n"," 0.855515  0.0149741  0.0193272     0.985822     0.941508    0.889608\n"," 0.93844   0.186831   0.238977      0.640169     0.511599    0.557855\n"," 0.165116  0.731223   0.333903      0.612696     0.943653    0.912044\n"," 0.603358  0.0966429  0.139028   …  0.351812     0.0522837   0.567665\n"," 0.192382  0.625075   0.902773      0.449006     0.684758    0.816761\n"," 0.267097  0.603497   0.152693      0.489982     0.776616    0.9623\n"," 0.940243  0.202874   0.363495      0.684509     0.283814    0.325294\n"," 0.231735  0.827416   0.932593      0.0870255    0.118691    0.0588176\n"," 0.132308  0.391922   0.164005   …  0.2868       0.151231    0.997276\n"," 0.772289  0.914671   0.0508106     0.457324     0.383207    0.714431\n"," 0.492924  0.184806   0.485501      0.814896     0.108223    0.903944\n"," 0.486994  0.472675   0.751939      0.528153     0.252313    0.8654"]},"metadata":{},"execution_count":16}],"source":["# Allocate our random matrix inputs and zero'd output\n","matrix_size = 1024\n","A = rand(matrix_size, matrix_size)\n","B = rand(matrix_size, matrix_size)\n","C = zeros(matrix_size, matrix_size)\n","\n","A"]},{"cell_type":"markdown","metadata":{"id":"DgIOf4Vnyxba"},"source":["The three nested loops implementation of matrix multiplication is easy to express on the CPU:"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"Ut1Ye6Ucyxba","executionInfo":{"status":"ok","timestamp":1744811344246,"user_tz":240,"elapsed":16,"user":{"displayName":"Rabab Alomairy","userId":"17426069845971301739"}},"outputId":"11c416a0-3485-4f98-9cab-68af4b081097"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["MatrixMultiplication! (generic function with 1 method)"]},"metadata":{},"execution_count":17}],"source":["function MatrixMultiplication!(C, A, B)\n","    for i in 1:size(C, 1)\n","        for j in 1:size(C, 2)\n","            C[i, j] = 0\n","            for k in 1:size(A, 2)\n","                C[i, j] += A[i, k] * B[k, j]\n","            end\n","        end\n","    end\n","end\n"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"EHq1FEtYyxbb","executionInfo":{"status":"ok","timestamp":1744811349861,"user_tz":240,"elapsed":3815,"user":{"displayName":"Rabab Alomairy","userId":"17426069845971301739"}},"outputId":"ace26bc9-fc45-4321-8aa2-51f30f2e0469"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["1024×1024 Matrix{Float64}:\n"," 245.013  240.989  248.09   251.864  …  243.379  252.709  246.208  245.343\n"," 248.218  247.06   247.529  250.108     247.16   260.909  252.556  245.11\n"," 259.227  255.325  253.269  256.752     257.718  264.649  258.87   251.411\n"," 254.971  254.935  252.419  257.587     257.727  264.72   256.444  245.325\n"," 252.853  250.352  249.558  248.129     250.415  254.944  256.359  245.326\n"," 243.896  242.375  239.664  242.608  …  242.506  248.201  250.104  238.062\n"," 244.934  240.909  246.583  249.87      248.93   254.613  252.11   241.863\n"," 255.599  254.366  259.545  255.809     255.401  263.658  257.591  249.531\n"," 254.695  245.282  250.888  249.315     243.945  256.361  255.367  245.544\n"," 252.042  255.34   249.833  250.79      250.995  255.279  254.641  246.194\n"," 257.263  254.936  260.803  264.896  …  262.311  264.158  263.657  252.691\n"," 253.08   248.461  247.256  252.968     251.76   258.748  254.374  242.948\n"," 253.739  253.971  249.772  250.995     254.165  263.794  256.429  244.85\n","   ⋮                                 ⋱    ⋮                        \n"," 246.799  245.139  245.414  241.35      250.296  256.593  246.279  244.894\n"," 241.014  236.747  237.26   239.921     239.452  246.556  243.297  234.08\n"," 251.806  252.116  249.619  258.406     256.923  261.888  258.462  248.332\n"," 251.595  252.331  246.75   251.157  …  250.291  254.934  254.548  248.999\n"," 262.191  260.475  256.821  259.899     262.035  267.675  271.439  250.309\n"," 251.261  247.383  245.841  252.428     251.98   259.619  250.594  244.056\n"," 250.923  248.663  246.818  252.05      249.526  260.335  256.573  243.108\n"," 262.43   256.001  252.507  262.71      260.59   268.137  265.537  255.926\n"," 272.015  270.321  266.163  262.192  …  268.333  269.169  267.514  258.82\n"," 251.497  248.768  247.983  247.434     252.527  254.18   252.717  246.78\n"," 261.186  261.136  258.448  256.858     261.125  266.964  261.15   250.826\n"," 251.166  252.086  249.564  253.543     254.519  261.897  256.444  248.412"]},"metadata":{},"execution_count":18}],"source":["MatrixMultiplication!(C, A, B)\n","C"]},{"cell_type":"markdown","metadata":{"id":"B4CGt8yCyxbb"},"source":["Of course, Julia has this one built-in already (it calls BLAS):"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"zOHVjhmSyxbb","executionInfo":{"status":"ok","timestamp":1744811354960,"user_tz":240,"elapsed":1190,"user":{"displayName":"Rabab Alomairy","userId":"17426069845971301739"}},"outputId":"00ac820c-9adf-4c5b-9e81-18bd3fd8f660"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["1024×1024 Matrix{Float64}:\n"," 245.013  240.989  248.09   251.864  …  243.379  252.709  246.208  245.343\n"," 248.218  247.06   247.529  250.108     247.16   260.909  252.556  245.11\n"," 259.227  255.325  253.269  256.752     257.718  264.649  258.87   251.411\n"," 254.971  254.935  252.419  257.587     257.727  264.72   256.444  245.325\n"," 252.853  250.352  249.558  248.129     250.415  254.944  256.359  245.326\n"," 243.896  242.375  239.664  242.608  …  242.506  248.201  250.104  238.062\n"," 244.934  240.909  246.583  249.87      248.93   254.613  252.11   241.863\n"," 255.599  254.366  259.545  255.809     255.401  263.658  257.591  249.531\n"," 254.695  245.282  250.888  249.315     243.945  256.361  255.367  245.544\n"," 252.042  255.34   249.833  250.79      250.995  255.279  254.641  246.194\n"," 257.263  254.936  260.803  264.896  …  262.311  264.158  263.657  252.691\n"," 253.08   248.461  247.256  252.968     251.76   258.748  254.374  242.948\n"," 253.739  253.971  249.772  250.995     254.165  263.794  256.429  244.85\n","   ⋮                                 ⋱    ⋮                        \n"," 246.799  245.139  245.414  241.35      250.296  256.593  246.279  244.894\n"," 241.014  236.747  237.26   239.921     239.452  246.556  243.297  234.08\n"," 251.806  252.116  249.619  258.406     256.923  261.888  258.462  248.332\n"," 251.595  252.331  246.75   251.157  …  250.291  254.934  254.548  248.999\n"," 262.191  260.475  256.821  259.899     262.035  267.675  271.439  250.309\n"," 251.261  247.383  245.841  252.428     251.98   259.619  250.594  244.056\n"," 250.923  248.663  246.818  252.05      249.526  260.335  256.573  243.108\n"," 262.43   256.001  252.507  262.71      260.59   268.137  265.537  255.926\n"," 272.015  270.321  266.163  262.192  …  268.333  269.169  267.514  258.82\n"," 251.497  248.768  247.983  247.434     252.527  254.18   252.717  246.78\n"," 261.186  261.136  258.448  256.858     261.125  266.964  261.15   250.826\n"," 251.166  252.086  249.564  253.543     254.519  261.897  256.444  248.412"]},"metadata":{},"execution_count":19}],"source":["C = A * B"]},{"cell_type":"markdown","metadata":{"id":"5m9lMQK8yxbb"},"source":["Our implementation performs quite a bit worse than Julia's (OpenBLAS), but that's OK - we haven't really optimized it at all, since this is just a tutorial:"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"TqecQQnyyxbc","executionInfo":{"status":"ok","timestamp":1744811384520,"user_tz":240,"elapsed":28834,"user":{"displayName":"Rabab Alomairy","userId":"17426069845971301739"}},"outputId":"427df7e3-c20b-48d4-eb98-a1dd43c2690d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["BenchmarkTools.Trial: 2 samples with 1 evaluation per sample.\n"," Range \u001b[90m(\u001b[39m\u001b[36m\u001b[1mmin\u001b[22m\u001b[39m … \u001b[35mmax\u001b[39m\u001b[90m):  \u001b[39m\u001b[36m\u001b[1m3.819 s\u001b[22m\u001b[39m … \u001b[35m  3.872 s\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmin … max\u001b[90m): \u001b[39m0.00% … 0.00%\n"," Time  \u001b[90m(\u001b[39m\u001b[34m\u001b[1mmedian\u001b[22m\u001b[39m\u001b[90m):     \u001b[39m\u001b[34m\u001b[1m3.845 s              \u001b[22m\u001b[39m\u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmedian\u001b[90m):    \u001b[39m0.00%\n"," Time  \u001b[90m(\u001b[39m\u001b[32m\u001b[1mmean\u001b[22m\u001b[39m ± \u001b[32mσ\u001b[39m\u001b[90m):   \u001b[39m\u001b[32m\u001b[1m3.845 s\u001b[22m\u001b[39m ± \u001b[32m37.426 ms\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmean ± σ\u001b[90m):  \u001b[39m0.00% ± 0.00%\n","\n","  \u001b[34m█\u001b[39m\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[32m \u001b[39m\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m█\u001b[39m \u001b[39m \n","  \u001b[34m█\u001b[39m\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[32m▁\u001b[39m\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m█\u001b[39m \u001b[39m▁\n","  3.82 s\u001b[90m         Histogram: frequency by time\u001b[39m        3.87 s \u001b[0m\u001b[1m<\u001b[22m\n","\n"," Memory estimate\u001b[90m: \u001b[39m\u001b[33m0 bytes\u001b[39m, allocs estimate\u001b[90m: \u001b[39m\u001b[33m0\u001b[39m."]},"metadata":{},"execution_count":20}],"source":["@benchmark MatrixMultiplication!(C, A, B)"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"yPdB0ur-yxbc","executionInfo":{"status":"ok","timestamp":1744811395300,"user_tz":240,"elapsed":10781,"user":{"displayName":"Rabab Alomairy","userId":"17426069845971301739"}},"outputId":"016581de-3df8-43fd-ee8d-29a5141c25c7"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["BenchmarkTools.Trial: 597 samples with 1 evaluation per sample.\n"," Range \u001b[90m(\u001b[39m\u001b[36m\u001b[1mmin\u001b[22m\u001b[39m … \u001b[35mmax\u001b[39m\u001b[90m):  \u001b[39m\u001b[36m\u001b[1m6.956 ms\u001b[22m\u001b[39m … \u001b[35m16.250 ms\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmin … max\u001b[90m): \u001b[39m0.00% … 7.90%\n"," Time  \u001b[90m(\u001b[39m\u001b[34m\u001b[1mmedian\u001b[22m\u001b[39m\u001b[90m):     \u001b[39m\u001b[34m\u001b[1m7.223 ms              \u001b[22m\u001b[39m\u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmedian\u001b[90m):    \u001b[39m0.00%\n"," Time  \u001b[90m(\u001b[39m\u001b[32m\u001b[1mmean\u001b[22m\u001b[39m ± \u001b[32mσ\u001b[39m\u001b[90m):   \u001b[39m\u001b[32m\u001b[1m8.351 ms\u001b[22m\u001b[39m ± \u001b[32m 1.920 ms\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmean ± σ\u001b[90m):  \u001b[39m1.74% ± 4.95%\n","\n","  \u001b[39m▂\u001b[39m█\u001b[34m▅\u001b[39m\u001b[39m▁\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[32m \u001b[39m\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m▁\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m▂\u001b[39m▂\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \n","  \u001b[39m█\u001b[39m█\u001b[34m█\u001b[39m\u001b[39m█\u001b[39m▇\u001b[39m█\u001b[39m▅\u001b[39m▆\u001b[39m▅\u001b[39m▁\u001b[39m▄\u001b[32m▄\u001b[39m\u001b[39m▅\u001b[39m▁\u001b[39m▅\u001b[39m▁\u001b[39m▅\u001b[39m▄\u001b[39m▄\u001b[39m▄\u001b[39m█\u001b[39m▇\u001b[39m▄\u001b[39m▄\u001b[39m▅\u001b[39m▁\u001b[39m▆\u001b[39m▄\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m▅\u001b[39m▆\u001b[39m█\u001b[39m▆\u001b[39m▅\u001b[39m▅\u001b[39m█\u001b[39m█\u001b[39m▅\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▄\u001b[39m▅\u001b[39m▁\u001b[39m▄\u001b[39m▄\u001b[39m▁\u001b[39m▁\u001b[39m▄\u001b[39m▁\u001b[39m▁\u001b[39m▄\u001b[39m \u001b[39m▆\n","  6.96 ms\u001b[90m      \u001b[39m\u001b[90mHistogram: \u001b[39m\u001b[90m\u001b[1mlog(\u001b[22m\u001b[39m\u001b[90mfrequency\u001b[39m\u001b[90m\u001b[1m)\u001b[22m\u001b[39m\u001b[90m by time\u001b[39m     14.6 ms \u001b[0m\u001b[1m<\u001b[22m\n","\n"," Memory estimate\u001b[90m: \u001b[39m\u001b[33m8.00 MiB\u001b[39m, allocs estimate\u001b[90m: \u001b[39m\u001b[33m2\u001b[39m."]},"metadata":{},"execution_count":21}],"source":["@benchmark A * B"]},{"cell_type":"markdown","metadata":{"id":"Xkgha_tZyxbc"},"source":["Ok, let's now implement matrix multiplication on the GPU:"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"-P7jysugyxbc","executionInfo":{"status":"ok","timestamp":1744811398520,"user_tz":240,"elapsed":558,"user":{"displayName":"Rabab Alomairy","userId":"17426069845971301739"}},"outputId":"3ac7716f-ab2f-4d15-dec6-815fc996948e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["1024×1024 CuArray{Float64, 2, CUDA.DeviceMemory}:\n"," 0.111411  0.0846486  0.0497382  …  0.613677     0.377845    0.0341445\n"," 0.174748  0.350421   0.227078      0.671424     0.141305    0.147167\n"," 0.731431  0.185005   0.503551      0.148479     0.880455    0.0922076\n"," 0.690734  0.108755   0.705794      0.825014     0.0326488   0.471357\n"," 0.84956   0.149493   0.163321      0.71118      0.736062    0.747991\n"," 0.785406  0.270562   0.0286078  …  0.903958     0.225837    0.322484\n"," 0.33145   0.667579   0.498192      0.442454     0.24898     0.910926\n"," 0.763405  0.352141   0.958001      0.000287206  0.353318    0.513929\n"," 0.684119  0.0739164  0.41486       0.47827      0.875003    0.61574\n"," 0.529939  0.792093   0.336086      0.595774     0.529696    0.320064\n"," 0.99392   0.917188   0.309304   …  0.230307     0.189118    0.443715\n"," 0.250578  0.451906   0.962513      0.174126     0.885104    0.569134\n"," 0.946768  0.228864   0.445785      0.279041     0.00986295  0.569224\n"," ⋮                               ⋱                           \n"," 0.855515  0.0149741  0.0193272     0.985822     0.941508    0.889608\n"," 0.93844   0.186831   0.238977      0.640169     0.511599    0.557855\n"," 0.165116  0.731223   0.333903      0.612696     0.943653    0.912044\n"," 0.603358  0.0966429  0.139028   …  0.351812     0.0522837   0.567665\n"," 0.192382  0.625075   0.902773      0.449006     0.684758    0.816761\n"," 0.267097  0.603497   0.152693      0.489982     0.776616    0.9623\n"," 0.940243  0.202874   0.363495      0.684509     0.283814    0.325294\n"," 0.231735  0.827416   0.932593      0.0870255    0.118691    0.0588176\n"," 0.132308  0.391922   0.164005   …  0.2868       0.151231    0.997276\n"," 0.772289  0.914671   0.0508106     0.457324     0.383207    0.714431\n"," 0.492924  0.184806   0.485501      0.814896     0.108223    0.903944\n"," 0.486994  0.472675   0.751939      0.528153     0.252313    0.8654"]},"metadata":{},"execution_count":22}],"source":["# We need first to move A and B matrices to the GPU and define a new DC zero'd matrix on the GPU\n","DA = CuArray(A)\n","DB = CuArray(B)\n","DC = CUDA.zeros(size(A))\n","\n","DA"]},{"cell_type":"markdown","metadata":{"id":"zDzqZJrNyxbc"},"source":["In the same way, here we can multiply the `DA` matrix by `DB` matrix using the `*` operator (which forwards the call to CUBLAS), thanks again to Julia's multiple dispatch:"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"cdoOelgjyxbd","executionInfo":{"status":"ok","timestamp":1744811402492,"user_tz":240,"elapsed":1715,"user":{"displayName":"Rabab Alomairy","userId":"17426069845971301739"}},"outputId":"5335a3aa-b0e7-445b-fbfb-336378a68603"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["1024×1024 CuArray{Float64, 2, CUDA.DeviceMemory}:\n"," 245.013  240.989  248.09   251.864  …  243.379  252.709  246.208  245.343\n"," 248.218  247.06   247.529  250.108     247.16   260.909  252.556  245.11\n"," 259.227  255.325  253.269  256.752     257.718  264.649  258.87   251.411\n"," 254.971  254.935  252.419  257.587     257.727  264.72   256.444  245.325\n"," 252.853  250.352  249.558  248.129     250.415  254.944  256.359  245.326\n"," 243.896  242.375  239.664  242.608  …  242.506  248.201  250.104  238.062\n"," 244.934  240.909  246.583  249.87      248.93   254.613  252.11   241.863\n"," 255.599  254.366  259.545  255.809     255.401  263.658  257.591  249.531\n"," 254.695  245.282  250.888  249.315     243.945  256.361  255.367  245.544\n"," 252.042  255.34   249.833  250.79      250.995  255.279  254.641  246.194\n"," 257.263  254.936  260.803  264.896  …  262.311  264.158  263.657  252.691\n"," 253.08   248.461  247.256  252.968     251.76   258.748  254.374  242.948\n"," 253.739  253.971  249.772  250.995     254.165  263.794  256.429  244.85\n","   ⋮                                 ⋱    ⋮                        \n"," 246.799  245.139  245.414  241.35      250.296  256.593  246.279  244.894\n"," 241.014  236.747  237.26   239.921     239.452  246.556  243.297  234.08\n"," 251.806  252.116  249.619  258.406     256.923  261.888  258.462  248.332\n"," 251.595  252.331  246.75   251.157  …  250.291  254.934  254.548  248.999\n"," 262.191  260.475  256.821  259.899     262.035  267.675  271.439  250.309\n"," 251.261  247.383  245.841  252.428     251.98   259.619  250.594  244.056\n"," 250.923  248.663  246.818  252.05      249.526  260.335  256.573  243.108\n"," 262.43   256.001  252.507  262.71      260.59   268.137  265.537  255.926\n"," 272.015  270.321  266.163  262.192  …  268.333  269.169  267.514  258.82\n"," 251.497  248.768  247.983  247.434     252.527  254.18   252.717  246.78\n"," 261.186  261.136  258.448  256.858     261.125  266.964  261.15   250.826\n"," 251.166  252.086  249.564  253.543     254.519  261.897  256.444  248.412"]},"metadata":{},"execution_count":23}],"source":["DC = DA * DB"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"BzJwnNf5yxbd","executionInfo":{"status":"ok","timestamp":1744811407165,"user_tz":240,"elapsed":9,"user":{"displayName":"Rabab Alomairy","userId":"17426069845971301739"}},"outputId":"179903b6-39e1-45eb-ccfa-90e3daa0117f"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["MatrixMultiplication_cuda! (generic function with 1 method)"]},"metadata":{},"execution_count":25}],"source":["function MatrixMultiplication_cuda!(C, A, B)\n","    # Calculate the global row and column indices\n","    row = (blockIdx().x - 1) * blockDim().x + threadIdx().x\n","    col = (blockIdx().y - 1) * blockDim().y + threadIdx().y\n","\n","    # Create a 0 of the same type as C's element type (for type stability)\n","    sum = zero(eltype(C))\n","\n","    if row <= size(A, 1) && col < size(B, 2)\n","        for i in 1:size(A, 2)\n","            # @inbounds disables bounds checking for array accesses, to improve performance\n","            # Note that incorrect usage can result in segfaults/memory faults/wrong results\n","            @inbounds sum += A[row, i] * B[i, col]\n","        end\n","        C[row, col] = sum\n","    end\n","\n","    return\n","end"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"AxFzi4SFyxbd","executionInfo":{"status":"ok","timestamp":1744811414871,"user_tz":240,"elapsed":658,"user":{"displayName":"Rabab Alomairy","userId":"17426069845971301739"}},"outputId":"38ee63af-6873-4668-bc5c-a77963940319"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["1024×1024 CuArray{Float64, 2, CUDA.DeviceMemory}:\n"," 245.013  240.989  248.09   251.864  …  243.379  252.709  246.208  245.343\n"," 248.218  247.06   247.529  250.108     247.16   260.909  252.556  245.11\n"," 259.227  255.325  253.269  256.752     257.718  264.649  258.87   251.411\n"," 254.971  254.935  252.419  257.587     257.727  264.72   256.444  245.325\n"," 252.853  250.352  249.558  248.129     250.415  254.944  256.359  245.326\n"," 243.896  242.375  239.664  242.608  …  242.506  248.201  250.104  238.062\n"," 244.934  240.909  246.583  249.87      248.93   254.613  252.11   241.863\n"," 255.599  254.366  259.545  255.809     255.401  263.658  257.591  249.531\n"," 254.695  245.282  250.888  249.315     243.945  256.361  255.367  245.544\n"," 252.042  255.34   249.833  250.79      250.995  255.279  254.641  246.194\n"," 257.263  254.936  260.803  264.896  …  262.311  264.158  263.657  252.691\n"," 253.08   248.461  247.256  252.968     251.76   258.748  254.374  242.948\n"," 253.739  253.971  249.772  250.995     254.165  263.794  256.429  244.85\n","   ⋮                                 ⋱    ⋮                        \n"," 246.799  245.139  245.414  241.35      250.296  256.593  246.279  244.894\n"," 241.014  236.747  237.26   239.921     239.452  246.556  243.297  234.08\n"," 251.806  252.116  249.619  258.406     256.923  261.888  258.462  248.332\n"," 251.595  252.331  246.75   251.157  …  250.291  254.934  254.548  248.999\n"," 262.191  260.475  256.821  259.899     262.035  267.675  271.439  250.309\n"," 251.261  247.383  245.841  252.428     251.98   259.619  250.594  244.056\n"," 250.923  248.663  246.818  252.05      249.526  260.335  256.573  243.108\n"," 262.43   256.001  252.507  262.71      260.59   268.137  265.537  255.926\n"," 272.015  270.321  266.163  262.192  …  268.333  269.169  267.514  258.82\n"," 251.497  248.768  247.983  247.434     252.527  254.18   252.717  246.78\n"," 261.186  261.136  258.448  256.858     261.125  266.964  261.15   250.826\n"," 251.166  252.086  249.564  253.543     254.519  261.897  256.444  248.412"]},"metadata":{},"execution_count":26}],"source":["# Split blocks up into 32x32 tiles\n","@cuda threads=(32, 32) blocks=(matrix_size ÷ 32, matrix_size ÷ 32) MatrixMultiplication_cuda!(DC, DA, DB)\n","\n","DC"]},{"cell_type":"markdown","metadata":{"id":"c6QWvU_myxbd"},"source":["And as we'd expect, the optimized CUBLAS implementation is far faster than ours:"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"CCzOBInAyxbd","executionInfo":{"status":"ok","timestamp":1744811423872,"user_tz":240,"elapsed":7027,"user":{"displayName":"Rabab Alomairy","userId":"17426069845971301739"}},"outputId":"9e7bb970-4ea7-4e9d-eb86-62cfe8fbd273"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["BenchmarkTools.Trial: 10000 samples with 1 evaluation per sample.\n"," Range \u001b[90m(\u001b[39m\u001b[36m\u001b[1mmin\u001b[22m\u001b[39m … \u001b[35mmax\u001b[39m\u001b[90m):  \u001b[39m\u001b[36m\u001b[1m 38.058 μs\u001b[22m\u001b[39m … \u001b[35m 22.594 ms\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmin … max\u001b[90m): \u001b[39m0.00% … 23.21%\n"," Time  \u001b[90m(\u001b[39m\u001b[34m\u001b[1mmedian\u001b[22m\u001b[39m\u001b[90m):     \u001b[39m\u001b[34m\u001b[1m130.422 μs               \u001b[22m\u001b[39m\u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmedian\u001b[90m):    \u001b[39m0.00%\n"," Time  \u001b[90m(\u001b[39m\u001b[32m\u001b[1mmean\u001b[22m\u001b[39m ± \u001b[32mσ\u001b[39m\u001b[90m):   \u001b[39m\u001b[32m\u001b[1m126.368 μs\u001b[22m\u001b[39m ± \u001b[32m316.869 μs\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmean ± σ\u001b[90m):  \u001b[39m0.83% ±  0.33%\n","\n","  \u001b[39m▅\u001b[39m▃\u001b[39m▁\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m▁\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[32m▄\u001b[39m\u001b[39m▆\u001b[39m█\u001b[34m▇\u001b[39m\u001b[39m▆\u001b[39m▄\u001b[39m▃\u001b[39m▂\u001b[39m▂\u001b[39m▁\u001b[39m \u001b[39m▂\n","  \u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m▇\u001b[39m▇\u001b[39m▆\u001b[39m▆\u001b[39m▆\u001b[39m▆\u001b[39m▆\u001b[39m▅\u001b[39m▄\u001b[39m▄\u001b[39m▃\u001b[39m▃\u001b[39m▁\u001b[39m▃\u001b[39m▁\u001b[39m▃\u001b[39m▃\u001b[39m▃\u001b[39m▃\u001b[39m▃\u001b[39m▃\u001b[39m▄\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▃\u001b[39m▁\u001b[39m▁\u001b[39m▃\u001b[39m▄\u001b[39m▄\u001b[39m▃\u001b[39m▁\u001b[39m▁\u001b[39m▃\u001b[39m▄\u001b[39m▄\u001b[39m▅\u001b[39m▄\u001b[39m▆\u001b[39m▆\u001b[39m▆\u001b[39m▇\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[32m█\u001b[39m\u001b[39m█\u001b[39m█\u001b[34m█\u001b[39m\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m \u001b[39m█\n","  38.1 μs\u001b[90m       \u001b[39m\u001b[90mHistogram: \u001b[39m\u001b[90m\u001b[1mlog(\u001b[22m\u001b[39m\u001b[90mfrequency\u001b[39m\u001b[90m\u001b[1m)\u001b[22m\u001b[39m\u001b[90m by time\u001b[39m        143 μs \u001b[0m\u001b[1m<\u001b[22m\n","\n"," Memory estimate\u001b[90m: \u001b[39m\u001b[33m1.62 KiB\u001b[39m, allocs estimate\u001b[90m: \u001b[39m\u001b[33m71\u001b[39m."]},"metadata":{},"execution_count":27}],"source":["@benchmark DA * DB"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"-z3LRCBnyxbd","executionInfo":{"status":"ok","timestamp":1744811434917,"user_tz":240,"elapsed":11039,"user":{"displayName":"Rabab Alomairy","userId":"17426069845971301739"}},"outputId":"09f660f1-a5b0-49b3-f3f2-e66ed3acdb16"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["BenchmarkTools.Trial: 4039 samples with 1 evaluation per sample.\n"," Range \u001b[90m(\u001b[39m\u001b[36m\u001b[1mmin\u001b[22m\u001b[39m … \u001b[35mmax\u001b[39m\u001b[90m):  \u001b[39m\u001b[36m\u001b[1m1.214 ms\u001b[22m\u001b[39m … \u001b[35m 1.329 ms\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmin … max\u001b[90m): \u001b[39m0.00% … 0.00%\n"," Time  \u001b[90m(\u001b[39m\u001b[34m\u001b[1mmedian\u001b[22m\u001b[39m\u001b[90m):     \u001b[39m\u001b[34m\u001b[1m1.228 ms              \u001b[22m\u001b[39m\u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmedian\u001b[90m):    \u001b[39m0.00%\n"," Time  \u001b[90m(\u001b[39m\u001b[32m\u001b[1mmean\u001b[22m\u001b[39m ± \u001b[32mσ\u001b[39m\u001b[90m):   \u001b[39m\u001b[32m\u001b[1m1.231 ms\u001b[22m\u001b[39m ± \u001b[32m10.203 μs\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmean ± σ\u001b[90m):  \u001b[39m0.00% ± 0.00%\n","\n","  \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m▂\u001b[39m▃\u001b[39m▇\u001b[39m▆\u001b[39m▇\u001b[39m█\u001b[39m▇\u001b[39m▅\u001b[34m▅\u001b[39m\u001b[39m▃\u001b[39m▄\u001b[32m▂\u001b[39m\u001b[39m▁\u001b[39m▂\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \n","  \u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▃\u001b[39m▄\u001b[39m▆\u001b[39m▇\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[34m█\u001b[39m\u001b[39m█\u001b[39m█\u001b[32m█\u001b[39m\u001b[39m█\u001b[39m█\u001b[39m▇\u001b[39m▆\u001b[39m▆\u001b[39m▅\u001b[39m▅\u001b[39m▅\u001b[39m▅\u001b[39m▄\u001b[39m▃\u001b[39m▄\u001b[39m▄\u001b[39m▃\u001b[39m▃\u001b[39m▃\u001b[39m▃\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m \u001b[39m▃\n","  1.21 ms\u001b[90m        Histogram: frequency by time\u001b[39m        1.27 ms \u001b[0m\u001b[1m<\u001b[22m\n","\n"," Memory estimate\u001b[90m: \u001b[39m\u001b[33m1.27 KiB\u001b[39m, allocs estimate\u001b[90m: \u001b[39m\u001b[33m36\u001b[39m."]},"metadata":{},"execution_count":28}],"source":["@benchmark CUDA.@sync @cuda threads=(32, 32) blocks=(matrix_size ÷ 32, matrix_size ÷ 32) MatrixMultiplication_cuda!(DC, DA, DB)"]},{"cell_type":"markdown","metadata":{"id":"RG_-yHSAyxbe"},"source":["Ouch! Why exactly is our kernel implementation slower?\n","\n","The answer is that this is only the naive implementation of matrix multiplication - if you did the same in CUDA C++, you'd get similarly bad performance. The performant implementation relies on tiling, where the matrix is divided into smaller submatrices (tiles) that fit more effectively within the GPU’s memory hierarchy, including shared memory and cache. Additionally, optimized kernels would use shared memory and WMMA instructions to greatly improve data locality and reduce bandwidth needs (both of which can be easily used in Julia).\n","\n","In an optimized implementation, each thread block on the GPU handles a specific tile of the output matrix, loading portions of the input tiles into shared memory to reduce the repeated global memory access. This approach enables a higher level of parallelism by allowing multiple tiles to be processed concurrently across the GPU cores, without bottlenecking on memory transfers.\n","\n","For the purpose of this tutorial, we will not be implementing these optimizations, but do know that they are as easy (or easier) to use in Julia compared to CUDA C++."]},{"cell_type":"markdown","metadata":{"id":"uT4V0oHkyxbe"},"source":["# KernelAbstractions"]},{"cell_type":"markdown","metadata":{"id":"2g2MX-kVyxbe"},"source":["Now that we know how to write vendor-specific kernels, let's explore the naive matrix multiplication example using KernelAbstractions.jl:"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"AWCFIYxkyxbe","outputId":"9a4ca7a7-c4ff-46ac-ca98-1ddd80f958b3"},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n","\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `/global/u2/t/train921/julia-hpc-tutorial-sc24-main/Project.toml`\n","\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `/global/u2/t/train921/julia-hpc-tutorial-sc24-main/Manifest.toml`\n"]}],"source":["import Pkg\n","Pkg.add(\"KernelAbstractions\")"]},{"cell_type":"markdown","metadata":{"id":"FNiVwOoRyxbe"},"source":["We'll also load the Random standard library to assist with certain random array initializations:"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"oeYPaOAHyxbf"},"outputs":[],"source":["using KernelAbstractions\n","using Random"]},{"cell_type":"markdown","metadata":{"id":"cuV4Novyyxbf"},"source":["Implementing a kernel with KernelAbstractions is very similar to implementing a kernel with CUDA.jl. The primary differences include annotating a kernel function with `@kernel`, and doing thread indexing using `@index` (which efficiently abstracts away the index calculations we were previously doing). Otherwise, most things are the same:"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"k_P6FWB2yxbf","outputId":"633008b7-1411-4b59-b060-ff9ce08a142c"},"outputs":[{"data":{"text/plain":["MatrixMultiplication_kernel! (generic function with 4 methods)"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["@kernel function MatrixMultiplication_kernel!(C, A, B)\n","    # Global index of each thread across multiple blocks in both x and y dimensions of the grid\n","    row, col = @index(Global, NTuple)\n","\n","    # Everything else is the same!\n","    sum = zero(eltype(C))\n","\n","    if row <= size(A, 1) && col <= size(B, 2)\n","        for i = 1:size(A, 2)\n","             @inbounds sum += A[row, i] * B[i, col]\n","        end\n","        @inbounds C[row, col] = sum\n","     end\n","end"]},{"cell_type":"markdown","metadata":{"id":"F-nzGy_dyxbf"},"source":["One key difference between KernelAbstractions and CUDA is that, because KernelAbstractions is portable, we need to select the CUDA \"backend\" when we compile our kernel (AMDGPU, Apple, and Intel are also supported). Most operations take the backend as the first argument, to allow Julia's multiple dispatch to redirect calls to the correct implementation. Additionally, KernelAbstractions separate the compilation and kernel launch stages, and provides configurations for each step to optimize further."]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"5VYtXBMwyxbf","outputId":"a896aa33-f33c-4c52-e746-cd6d07ac0cb1"},"outputs":[{"data":{"text/plain":["true"]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["# Select the CUDA backend\n","Backend = CUDA.CUDABackend()\n","\n","# Use KernelAbstractions's APIs to allocate GPU matrices DA, DB, and DC\n","matrix_size = 2048\n","T = Float64\n","DA = rand!(allocate(Backend, T, matrix_size, matrix_size))\n","DB = rand!(allocate(Backend, T, matrix_size, matrix_size))\n","DC = KernelAbstractions.zeros(Backend, T, matrix_size, matrix_size)\n","\n","# Compile the kernel\n","# We'll statically assign the workgroup (AKA block) size to allow for additional compile-time optimizations\n","workgroupsize = (32, 32)\n","kernel! = MatrixMultiplication_kernel!(Backend, workgroupsize)\n","\n","# Launch the kernel with our GPU matrices as inputs\n","kernel!(DC, DA, DB, ndrange=(size(DC)))\n","\n","# Explicitly wait for the kernel to complete\n","KernelAbstractions.synchronize(Backend)\n","\n","# Are our results what we'd expect to see (compared to CUBLAS)?\n","isapprox(DC, DA * DB)"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"FkAgS-bwyxbf","outputId":"e9e9b993-ce05-402c-a586-393d675d0606"},"outputs":[{"data":{"text/plain":["BenchmarkTools.Trial: 348 samples with 1 evaluation.\n"," Range \u001b[90m(\u001b[39m\u001b[36m\u001b[1mmin\u001b[22m\u001b[39m … \u001b[35mmax\u001b[39m\u001b[90m):  \u001b[39m\u001b[36m\u001b[1m13.950 ms\u001b[22m\u001b[39m … \u001b[35m14.059 ms\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmin … max\u001b[90m): \u001b[39m0.00% … 0.00%\n"," Time  \u001b[90m(\u001b[39m\u001b[34m\u001b[1mmedian\u001b[22m\u001b[39m\u001b[90m):     \u001b[39m\u001b[34m\u001b[1m14.013 ms              \u001b[22m\u001b[39m\u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmedian\u001b[90m):    \u001b[39m0.00%\n"," Time  \u001b[90m(\u001b[39m\u001b[32m\u001b[1mmean\u001b[22m\u001b[39m ± \u001b[32mσ\u001b[39m\u001b[90m):   \u001b[39m\u001b[32m\u001b[1m14.012 ms\u001b[22m\u001b[39m ± \u001b[32m20.283 μs\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmean ± σ\u001b[90m):  \u001b[39m0.00% ± 0.00%\n","\n","  \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m▁\u001b[39m \u001b[39m▃\u001b[39m▃\u001b[39m▂\u001b[39m▅\u001b[39m \u001b[39m \u001b[39m▄\u001b[39m▁\u001b[39m▂\u001b[39m▆\u001b[39m▅\u001b[39m▃\u001b[34m▄\u001b[39m\u001b[39m▂\u001b[39m▄\u001b[39m▄\u001b[39m▁\u001b[39m█\u001b[39m▄\u001b[39m▂\u001b[39m█\u001b[39m▄\u001b[39m▂\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \n","  \u001b[39m▄\u001b[39m▁\u001b[39m▁\u001b[39m▃\u001b[39m▁\u001b[39m▃\u001b[39m▃\u001b[39m▃\u001b[39m▁\u001b[39m▄\u001b[39m▅\u001b[39m▅\u001b[39m▄\u001b[39m▆\u001b[39m▁\u001b[39m▃\u001b[39m▃\u001b[39m▅\u001b[39m▅\u001b[39m▄\u001b[39m▆\u001b[39m▅\u001b[39m█\u001b[39m▆\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m▇\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[34m█\u001b[39m\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m▇\u001b[39m▅\u001b[39m▆\u001b[39m█\u001b[39m▇\u001b[39m▆\u001b[39m▇\u001b[39m▃\u001b[39m▅\u001b[39m▄\u001b[39m▅\u001b[39m▅\u001b[39m \u001b[39m▅\n","  14 ms\u001b[90m           Histogram: frequency by time\u001b[39m        14.1 ms \u001b[0m\u001b[1m<\u001b[22m\n","\n"," Memory estimate\u001b[90m: \u001b[39m\u001b[33m1.89 KiB\u001b[39m, allocs estimate\u001b[90m: \u001b[39m\u001b[33m60\u001b[39m."]},"execution_count":30,"metadata":{},"output_type":"execute_result"}],"source":["@benchmark begin\n","    kernel!(DC, DA, DB, ndrange=size(DC))\n","    KernelAbstractions.synchronize(Backend)\n","end"]},{"cell_type":"markdown","metadata":{"id":"0CcblgJpyxbg"},"source":["# Example: Memory copy with KernelAbstractions"]},{"cell_type":"markdown","metadata":{"id":"qFjeEzUvyxbg"},"source":["Let's now see a different kind of example, to show how to use shared memory in KernelAbstractions. This kernel performs a matrix copy using local memory (also known as shared memory in CUDA), which can significantly speed up the memory access times by reducing global memory bandwidth usage:"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"oaFGXVd5yxbg","outputId":"c7d6ad23-13e6-4a6b-b842-2a7debdffaaa"},"outputs":[{"data":{"text/plain":["lmem_copy_kernel! (generic function with 4 methods)"]},"execution_count":31,"metadata":{},"output_type":"execute_result"}],"source":["@kernel function lmem_copy_kernel!(output, @Const(input))\n","    # Gets the global index of the thread in a multidimensional grid, which is used to index into the global input and output arrays.\n","    I, J = @index(Global, NTuple)\n","    # Gets the local index within a thread block or workgroup, useful for indexing into locally shared memory.\n","    i, j = @index(Local, NTuple) # Local index of thread\n","\n","    # @groupsize() retrieves the dimensions of the thread block or workgroup.\n","    # The @uniform ensures that these values are treated as constants that are the same for all threads.\n","    N = @uniform @groupsize()[1] # same as blockDim().x\n","    M = @uniform @groupsize()[2] # same as blockDim().y\n","\n","    # Allocate local (shared) memory\n","    tile = @localmem eltype(output) (N, M)\n","\n","    # First, data from the global input array is loaded into the shared tile array using local indices.\n","    @inbounds tile[i, j] = input[I, J]\n","\n","    # @synchronize ensures that all threads in the workgroup have completed their memory writes to the shared memory before proceeding.\n","    # This is crucial to prevent race conditions.\n","    @synchronize\n","\n","    # Finally, the data is written back from the shared tile array to the global output array.\n","    @inbounds output[I, J] = tile[i, j]\n","end"]},{"cell_type":"markdown","metadata":{"id":"wCdBnA_6yxbg"},"source":["This kernel is a little bit more \"advanced\" than prior kernels, but still is quite readable once you know what each of the macros do. We can quickly test that it works correctly:"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"IFTpIlcEyxbg","outputId":"f86c8e76-0c91-4271-c6a4-3c4b4ae758fd"},"outputs":[{"data":{"text/plain":["true"]},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["# Allocate inputs and outputs\n","input = rand!(allocate(Backend, T, matrix_size, matrix_size))\n","output = KernelAbstractions.zeros(Backend, T, matrix_size, matrix_size)\n","\n","# Compile and launch the kernel, and wait for it to complete\n","lmem_copy! = lmem_copy_kernel!(Backend, workgroupsize)\n","lmem_copy!(output, input, ndrange=size(input))\n","KernelAbstractions.synchronize(Backend)\n","\n","# Confirm that the output matrix now matches the input matrix\n","all(input == output)"]}],"metadata":{"kernelspec":{"display_name":"Julia","name":"julia"},"language_info":{"name":"julia"},"colab":{"provenance":[{"file_id":"1d5DhDmU6-0YpmB6MNCoc_TlzTxwQs8Uw","timestamp":1744929744258},{"file_id":"1v8hCrhw5bow8VaPwvDCxGejAcTkopnkX","timestamp":1744833488330}],"machine_shape":"hm","gpuType":"A100"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}